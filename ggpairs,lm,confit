---
title: "STAT "
author: ""
date: ""
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Exercise 1

Derive the least squares estimates for $\beta_0$ and $\beta_1$ in the simple linear regression model.  That is, show that \[\sum_{i=1}^n(y_i-(b_0+b_1x))^2\] is minimized when \[b_1=\frac{SS_{xy}}{SS_{x}}=\frac{\sum_{i=1}^n x_iy_i-\frac{1}{n}\sum_{i=1}^nx_i\sum_{i=1}^ny_i}{\sum_{i=1}^nx_i^2-\frac{1}{n}\left(\sum_{i=1}^nx_i\right)^2} \] and \[b_0=\overline{y}-b_1\overline{x}.\]  

You can upload an image of your handwritten work.



## Exercise 2
Load the Survey package and access the apisrs data.  Check out all the variable definitions by putting apipop into the help search
a. Create a filtered data set containing only the elementary schools.  Call this data set apiel
b. Create two correlation matrices with ggpairs to look at how api00 relates to the other numeric variables in the data set.  One matrix should include api00 with columns 11, 20, 21, 23-26, and 34-36.  The other should include the parental education variables in columns 27-30. Do not consider api99, target, or growth in this analysis.  Comment on which variables are strongly associated and whether the association is positive or negative.
c. Create a linear regression model predicting api00 with meals.  Output the summary data.
d. Check the assumptions about residuals graphically.  Comment on whether the linear model seems appropriate here.
e. Conduct a hypothesis test of the null that the slope is zero versus the alternative that the slope is negative.  Include all steps of the hypothesis test.
f. Plot api00 versus meals using ggplot.  Include the confidence band for the line.  Add in the prediction ribbon for new observations using 95% confidence.
g. Compute a 90% prediction interval for a school where 50 percent of students receive meals.  Give a sentence interpreting this interval.
h. Identify the 5 schools with percent receiving meals over 50 that have the largest positive residuals.  These schools are doing well and may have programs others can emulate.


```{r ex2, warnings=F}
library(tidyverse)
library(GGally)
library(survey)
data(api, package="survey")
View(apisrs)

#a
apiel <- apisrs %>%
 filter(stype=="E")


#b
ggpairs(apiel, columns = c(11,12, 20, 21, 23:26, 34:36))
ggpairs(apiel, columns = c(12,27:30))



#c
api00lm <- lm(api00~meals, data=apiel)
summary(api00lm)

#d

ggplot(apiel, aes(x=meals, y=api00))+
  geom_point()+
  geom_smooth(method=lm)
#There is some sort of balance when it comes to residulas we see that there is points that are up and down and some that are low due to this making this a normal residual plot.This seems like an apporia  since they are normally dis plot 

#e

help("t.test")

  summary(apielmodel)
pt(-23.46,140, lower.tail = F)
anova(apielmodel)

#1 ho:The slope of this will be zero  Ha:The slope will not be equal to zero
#2 using an anova test in order to prove what equals
#3 The value of the test statistic (F) is /MSB/mse=
#4 the pvalue of this will equal 
#5. Since p-value is less than 0.05, we will reject the null hypothesis and can say that thee is a d

#f
lwr <- predict(model1, newdata = apiel,interval = "confidence", level = .95) [,2]
upr <- predict(model1, newdata = apiel,interval = "confidence", level = .95) [,3]
 ggplot(apiel, aes(x=meals, y=api00))+
  geom_point()+
  geom_line(aes(y=lwr),color="red",linetype="dashed")+
  geom_line(aes(y=upr),color="red",linetype="dashed")+
  stat_smooth(method=lm,se=T)+
  geom_ribbon(aes(ymin = lwr, ymax = upr), fill = "lightblue2", alpha =0.2) 
#g
d <- data.frame(meals=50)
predict(api00lm, newdata = d, interval = 'prediction', level=0.90)

#The interval shows that there is at least 50 percent of those who reaa a meal is at its predeicaiton valuel

#h
head(sort(residuals(model1)[apiel$meals>50],decreasing = T),5)
apiel$name[which(rownames(apiel)%in% c(1273,2295,3764,837,3412))]
```



## Exercise 3

Continuing with the apiel dataset, consider predicting ell with meals. The relationship between meals and ell shows some nonlinearity.  Find a transformation that produces a higher R^2 value without causing problems in the residuals.  Verify this graphically and comment on the improvement.  Note that if you add a new (transformed) variable to the data set, it's fine to use this new name in the formula.  If you include the math operators for the trasformation in your formula, you should use the AsIs operator I() around your expression so the math operators are not misinterpreted in the formula.


```{r}

ell_lm <- lm(ell~meals,data=apiel)
summary(ell_lm)

plot(apiel$meals, apiel$ell)

elldf <- apiel %>%
 select(ell, meals)
elldf <- mutate(elldf, sqr_ell=sqrt(ell), sq_ell=ell^2, e_ell=exp(ell), sqr_meals=sqrt(meals), e_meals=exp(meals)
, sq_meals=meals^2 )
ggpairs(elldf) 

ell_lmt <- lm(sqrt(ell)~(meals),data=apiel)
summary(ell_lmt) 

ggplot(elldf, aes(x=meals, y=sqr_ell))+geom_point()+geom_smooth() 
plot(apiel$meals, resid(ell_lmt))
```
## Exercise 4

a. Fit a linear model called edmodel predicting api00 with average parent education (avg.ed).
b. Give 95% confidence intervals for the slopes and intercepts of this model.  The matrix summary(edmodel)$coefficients has the estimates in the first column and their standard errors in the second column.
c. Evalute the fit and predictive ability of this model.  ie, how far off do you expect predictions to be on average, and what fraction of the variability in api00 can be explained by the regression on avg.ed?
d. Calculate $(X'X)$ for this model.  Identify what values fall in each of the four slots  (what formula would go in each generally)?

```{r}
#a
edmodel <- lm(api00~avg.ed, data=apiel)

   
   #b
summary(edmodel)$coefficients
confint(edmodel, level=0.95)
 
   #c We see that the expected predictions will be on average and that means that its variability will also be close to the regression when it comes to the avg.ed intercept of this model that is chosen from the columns.75.43% of the variability in api00 can be explained by the regression on avg.ed based on this model.

#d

X <- na.omit(apiel$avg.ed)
t(X)%*%X
  

```

